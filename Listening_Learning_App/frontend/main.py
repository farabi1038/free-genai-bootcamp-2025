"""
Main entry point for the Listening Learning App frontend

This version uses direct imports but keeps the code clean and modular.
"""

import streamlit as st
import traceback
import logging
import requests
import sys
import os
from pathlib import Path
import json
import time
import socket
import re
from youtube_transcript_api import YouTubeTranscriptApi

# Fix import path
# Get the path to the parent directory of the Listening_Learning_App folder
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent  # Go up to Listening_Learning_App directory
sys.path.insert(0, str(project_root))  # Add to Python path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# App states
APP_STATES = {
    "HOME": "home",
    "VIDEO_SELECTION": "video_selection",
    "PRACTICE": "practice", 
    "REVIEW": "review",
    "PROCESSING_VIDEO": "processing_video",
    "AUDIO_EXERCISE": "audio_exercise",
    "AUDIO_EXERCISE_REVIEW": "audio_exercise_review"
}

# Additional app states
APP_STATE_EXTRACT_QUESTIONS = "EXTRACT_QUESTIONS"

# Function to check for Ollama availability
def check_ollama_availability():
    """Check if Ollama is available and set session state accordingly"""
    logger.info("Checking Ollama availability...")
    
    # First try to get the preferred model from config
    preferred_model = "llama3.2:1b"
    try:
        config_path = Path(__file__).parent.parent / "config.json"
        if config_path.exists():
            with open(config_path, 'r') as f:
                config = json.load(f)
                if "ollama_model" in config:
                    preferred_model = config["ollama_model"]
                    logger.info(f"Found preferred Ollama model in config: {preferred_model}")
    except Exception as e:
        logger.warning(f"Error reading Ollama model from config: {e}")
    
    try:
        # Try to get available models from Ollama
        try:
            # Try tags endpoint first which shows available models
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                st.session_state["ollama_available"] = True
                
                # Parse the models from the response
                models_data = response.json()
                available_models = [model["name"] for model in models_data.get("models", [])]
                logger.info(f"Available Ollama models: {available_models}")
                
                # Set the model in the session state
                if available_models:
                    # Try to use the preferred model if available
                    if preferred_model in available_models:
                        st.session_state["ollama_model"] = preferred_model
                    else:
                        # Otherwise use the first available model
                        st.session_state["ollama_model"] = available_models[0]
                    
                    logger.info(f"Using Ollama model: {st.session_state['ollama_model']}")
                else:
                    # No models available, but Ollama is running
                    st.session_state["ollama_model"] = preferred_model
                    logger.warning(f"No models available in Ollama. Using default: {preferred_model}")
                
                return
        except Exception as e:
            # If tags endpoint fails, try version endpoint
            logger.warning(f"Error checking Ollama tags: {e}")
            
            try:
                response = requests.get("http://localhost:11434/api/version", timeout=2)
                if response.status_code == 200:
                    # Ollama is running but we couldn't get models
                    st.session_state["ollama_available"] = True
                    st.session_state["ollama_model"] = preferred_model
                    logger.info(f"Ollama is available but couldn't get models. Using default: {preferred_model}")
                    return
            except Exception as e2:
                logger.warning(f"Error checking Ollama version: {e2}")
        
        # If we get here, Ollama is not available
        logger.warning("Ollama is not available - couldn't connect to API")
        st.session_state["ollama_available"] = False
        st.session_state["ollama_model"] = preferred_model  # Still store the preferred model
        
    except Exception as e:
        logger.error(f"Error in Ollama availability check: {str(e)}")
        st.session_state["ollama_available"] = False
        st.session_state["ollama_model"] = preferred_model

# Is port in use
def is_port_in_use(port):
    """Check if a port is in use"""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        return s.connect_ex(('localhost', port)) == 0

# Find backend server
def find_backend_server():
    """
    Find the backend server by checking common ports.
    Returns the backend URL if found, otherwise None.
    """
    logger.info("Searching for backend server...")
    backend_url = None
    
    # First try to read from config file
    try:
        config_path = Path(__file__).parent.parent / "config.json"
        if config_path.exists():
            with open(config_path, 'r') as f:
                config = json.load(f)
                backend_port = config.get('backend_port')
                if backend_port:
                    logger.info(f"Found backend port {backend_port} in config file")
                    # Try this port first
                    try:
                        url = f"http://localhost:{backend_port}/health"
                        response = requests.get(url, timeout=1)
                        if response.status_code == 200:
                            backend_url = f"http://localhost:{backend_port}"
                            logger.info(f"Successfully connected to backend at {backend_url}")
                            return backend_url
                    except:
                        logger.warning(f"Backend not found on configured port {backend_port}")
    except Exception as e:
        logger.warning(f"Could not read backend port from config: {e}")
    
    # Check common ports including the specific port 8040 used in your logs
    default_ports = [8040, 8000, 8080, 5000]
    
    # First check default ports
    for port in default_ports:
        try:
            url = f"http://localhost:{port}/health"
            response = requests.get(url, timeout=0.5)
            if response.status_code == 200:
                backend_url = f"http://localhost:{port}"
                logger.info(f"Found backend server at: {backend_url}")
                return backend_url
        except:
            continue
    
    # Narrow the port range to be more efficient
    # Focus on ports around 8040 where the backend is likely running
    range_ports = list(range(8030, 8050))
    logger.info(f"Checking port range {range_ports[0]}-{range_ports[-1]} for backend server...")
    
    for port in range_ports:
        try:
            url = f"http://localhost:{port}/health"
            response = requests.get(url, timeout=0.2)  # Use shorter timeout for range scan
            if response.status_code == 200:
                backend_url = f"http://localhost:{port}"
                logger.info(f"Found backend server at: {backend_url}")
                return backend_url
        except:
            continue

    if not backend_url:
        logger.warning("Could not find a running backend server")

    return backend_url

# Navigation functions
def go_to_home():
    """Navigate to home page"""
    st.session_state.app_state = APP_STATES["HOME"]

def go_to_video_selection():
    """Navigate to video selection page"""
    st.session_state.app_state = APP_STATES["VIDEO_SELECTION"]

def go_to_extract_questions():
    """Navigate to extract questions page"""
    st.session_state.app_state = APP_STATE_EXTRACT_QUESTIONS
    
    # Clear any previous extraction results when navigating to the page
    if "extracted_content" in st.session_state:
        del st.session_state["extracted_content"]

# Initialize session state
def initialize_session_state():
    """Initialize session state variables"""
    if "app_state" not in st.session_state:
        st.session_state["app_state"] = APP_STATES["HOME"]
        
    # Check if backend is running
    if "backend_available" not in st.session_state:
        st.session_state["backend_available"] = False
        
    # Check if Ollama is available
    if "ollama_available" not in st.session_state:
        st.session_state["ollama_available"] = False
        st.session_state["ollama_model"] = "mistral"

# Extract YouTube ID
def extract_youtube_id(url):
    """
    Extract the video ID from a YouTube URL
    
    Parameters:
        url (str): YouTube URL
    
    Returns:
        str: YouTube video ID, or None if not found
    """
    if not url:
        return None
        
    # Clean the URL first (trim whitespace, handle copy-paste issues)
    url = url.strip()
    
    # Handle common issues with copied URLs
    if url.startswith('"') and url.endswith('"'):
        url = url[1:-1]  # Remove quotes
        
    if url.startswith("'") and url.endswith("'"):
        url = url[1:-1]  # Remove quotes
        
    # YouTube URL patterns
    patterns = [
        r'(?:https?:\/\/)?(?:www\.)?(?:youtube\.com\/watch\?v=|youtu\.be\/)([^\/&\?#\s]+)',  # Standard and shortened
        r'(?:https?:\/\/)?(?:www\.)?youtube\.com\/embed\/([^\/&\?#\s]+)',  # Embed URL
        r'(?:https?:\/\/)?(?:www\.)?youtube\.com\/v\/([^\/&\?#\s]+)',      # Old embed URL
        r'(?:https?:\/\/)?(?:www\.)?youtube\.com\/user\/[^\/]+\/([^\/&\?#\s]+)', # User URL
        r'(?:https?:\/\/)?(?:www\.)?youtube\.com\/.*[?&]v=([^&\s]+)'       # Other formats with v parameter
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            video_id = match.group(1)
            # Validate the video ID format (should be 11 characters for standard videos)
            if 11 <= len(video_id) <= 12:
                return video_id
    
    # If no patterns match, this might be a direct video ID
    if 11 <= len(url) <= 12 and url.isalnum() or url.replace('-', '').replace('_', '').isalnum():
        return url
        
    return None

# Extract questions from YouTube
def extract_questions_from_youtube(video_url):
    """
    Extract questions from a YouTube video transcript
    
    Parameters:
        video_url (str): YouTube URL
    
    Returns:
        dict: Dictionary with extracted content (questions or conversations)
    """
    try:
        # Validate the YouTube URL
        video_id = extract_youtube_id(video_url)
        if not video_id:
            st.error("ç„¡åŠ¹ãªYouTube URLã§ã™ã€‚æœ‰åŠ¹ãªURLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚(Invalid YouTube URL. Please provide a valid URL.)")
            # Show examples of valid URLs
            st.info("æœ‰åŠ¹ãªURLä¾‹ (Valid URL examples):\n- https://www.youtube.com/watch?v=2aqVJS6QOoY\n- https://youtu.be/2aqVJS6QOoY")
            return None
            
        st.info(f"å‡¦ç†ä¸­ã®ãƒ“ãƒ‡ã‚ªID: {video_id} (Processing video ID: {video_id})")
        
        # Get transcript using youtube_transcript_api (open source)
        try:
            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['ja', 'ja-JP'])
            st.success("æ—¥æœ¬èªå­—å¹•ã‚’æ­£å¸¸ã«å–å¾—ã—ã¾ã—ãŸï¼(Japanese transcript successfully retrieved!)")
        except Exception as e:
            st.error(f"æ—¥æœ¬èªå­—å¹•ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)} (Failed to get Japanese transcript)")
            st.info("ã„ãšã‚Œã‹ã®åˆ©ç”¨å¯èƒ½ãªå­—å¹•ã‚’å–å¾—ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™... (Trying to get any available transcript...)")
            try:
                transcript = YouTubeTranscriptApi.get_transcript(video_id)
                st.success("å­—å¹•ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆéæ—¥æœ¬èªï¼‰ã€‚(Transcript retrieved (non-Japanese).)")
            except Exception as e2:
                st.error(f"ã„ãšã‚Œã®å­—å¹•ã‚‚å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {str(e2)} (Failed to get any transcript)")
                # Try to get auto-generated transcript as last resort
                try:
                    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['ja', 'en', 'auto'])
                    st.success("è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸå­—å¹•ã‚’å–å¾—ã—ã¾ã—ãŸã€‚(Auto-generated transcript retrieved.)")
                except Exception as e3:
                    st.error("ã“ã®ãƒ“ãƒ‡ã‚ªã«åˆ©ç”¨å¯èƒ½ãªå­—å¹•ãŒã‚ã‚Šã¾ã›ã‚“ã€‚(No transcript available for this video.)")
                    return None
            
        # Clean and format transcript for processing
        formatted_transcript = []
        for segment in transcript:
            # Clean the text - remove special characters and normalize
            text = segment["text"]
            # Replace common transcript artifacts
            text = text.replace("\n", " ")
            text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces with a single space
            
            # Remove non-Japanese artifacts (often seen in auto-generated transcripts)
            text = re.sub(r'[a-zA-Z]\s\d+', '', text)  # Remove patterns like "n 5"
            text = re.sub(r'\(\d+\)', '', text)  # Remove patterns like "(123)"
            
            formatted_transcript.append({
                "start": segment["start"],
                "text": text.strip(),
                "duration": segment["duration"]
            })
            
        # Show transcript preview for debugging
        with st.expander("å­—å¹•ã®å†…å®¹ (Transcript Content)", expanded=False):
            st.write(f"å–å¾—ã—ãŸå­—å¹•ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: {len(formatted_transcript)}å€‹ (Found {len(formatted_transcript)} transcript segments)")
            if len(formatted_transcript) > 0:
                st.write("å­—å¹•ã®å†…å®¹ (Transcript content):")
                for segment in formatted_transcript[:20]:  # Show first 20 segments to avoid overwhelming the UI
                    st.write(f"{segment['start']:.1f}s: {segment['text']}")
        
        # Combine segments into meaningful units for better context
        cleaned_transcript = []
        current_text = ""
        current_start = 0
        current_duration = 0
        
        for segment in formatted_transcript:
            # If the segment is very short, combine it with the previous one
            if len(current_text) == 0:
                # First segment
                current_text = segment["text"]
                current_start = segment["start"]
                current_duration = segment["duration"]
            elif len(segment["text"]) < 10 or len(current_text) < 20:
                # Combine short segments
                current_text += " " + segment["text"]
                current_duration += segment["duration"]
            else:
                # Add the current segment to the cleaned transcript
                cleaned_transcript.append({
                    "start": current_start,
                    "text": current_text,
                    "duration": current_duration
                })
                # Start a new segment
                current_text = segment["text"]
                current_start = segment["start"]
                current_duration = segment["duration"]
        
        # Add the last segment
        if current_text:
            cleaned_transcript.append({
                "start": current_start,
                "text": current_text,
                "duration": current_duration
            })
        
        # If cleaned transcript is too short, use the original
        if len(cleaned_transcript) < 5 and len(formatted_transcript) > 5:
            cleaned_transcript = formatted_transcript
        
        # Extract ONLY REAL questions from the transcript (questions actually asked in the video)
        actual_questions = []
        
        # Combine all transcript text for pattern matching
        full_text = " ".join([segment["text"] for segment in cleaned_transcript])
        st.info(f"å­—å¹•å†…ã®æ–‡å­—æ•°: {len(full_text)}æ–‡å­— (Transcript length: {len(full_text)} characters)")
        
        # Japanese question detection patterns
        question_patterns = [
            # Pattern for questions ending with ka (ã‹) and question mark
            r'([^ã€‚ï¼Ÿï¼]*[ã‹][ï¼Ÿ])',
            # Pattern for questions ending with ka (ã‹) and period
            r'([^ã€‚ï¼Ÿï¼]*[ã‹][ã€‚])',
            # Pattern for questions ending with a question mark
            r'([^ã€‚ï¼Ÿï¼]*[ï¼Ÿ])',
            # Pattern for polite questions
            r'([^ã€‚ï¼Ÿï¼]*(?:ã§ã™ã‹|ã¾ã™ã‹|ã®ã§ã™ã‹|ã®ã§ã—ã‚‡ã†ã‹)[ï¼Ÿã€‚]?)',
            # Pattern for questions with interrogatives
            r'([^ã€‚ï¼Ÿï¼]*(?:ä½•|ãªã«|ã©ã†|ãªãœ|ã©ã“|èª°|ã ã‚Œ|ã„ã¤|ã©ã‚“ãª|ã©ã®)[^ã€‚ï¼Ÿï¼]*[ã‹][ï¼Ÿã€‚]?)'
        ]
        
        # Process each segment for questions
        for segment_idx, segment in enumerate(cleaned_transcript):
            segment_text = segment["text"]
            
            # Apply each pattern to find questions in this segment
            for pattern in question_patterns:
                matches = re.finditer(pattern, segment_text)
                for match in matches:
                    question_text = match.group(0).strip()
                    
                    # Skip very short questions or duplicates
                    if len(question_text) < 10:
                        continue
                        
                    if question_text in [q["question_text"] for q in actual_questions]:
                        continue
                    
                    # Get context (surrounding segments)
                    # Get more context for better understanding
                    context_before = []
                    for i in range(max(0, segment_idx - 3), segment_idx):
                        if cleaned_transcript[i]["text"].strip():
                            context_before.append(cleaned_transcript[i]["text"])
                    
                    context_after = []
                    for i in range(segment_idx + 1, min(len(cleaned_transcript), segment_idx + 4)):
                        if cleaned_transcript[i]["text"].strip():
                            context_after.append(cleaned_transcript[i]["text"])
                    
                    # Join context with meaningful separators
                    context_before_text = " ... ".join(context_before) if context_before else ""
                    context_after_text = " ... ".join(context_after) if context_after else ""
                    
                    # Add the question
                    actual_questions.append({
                        "question_text": question_text,
                        "segment_start": segment["start"],
                        "segment_end": segment["start"] + segment["duration"],
                        "context_before": context_before_text,
                        "context_after": context_after_text,
                        "content_type": "è³ªå• (Question)"
                    })
        
        # Check if we found any questions
        if not actual_questions:
            st.warning("å‹•ç”»ã‹ã‚‰ç›´æ¥è³ªå•ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚(No direct questions found in the video.)")
            
            # Extract conversation segments as a fallback
            conversations = extract_conversations(cleaned_transcript, min_length=50)
            
            # If we have conversations, show them
            if conversations:
                st.success(f"{len(conversations)}å€‹ã®ä¼šè©±ã‚’è¦‹ã¤ã‘ã¾ã—ãŸã€‚(Found {len(conversations)} conversations.)")
                # Add content type to each conversation
                for conversation in conversations:
                    conversation["content_type"] = "ä¼šè©± (Conversation)"
                return {
                    "type": "conversations",
                    "conversations": conversations
                }
            else:
                st.error("ã“ã®å‹•ç”»ã‹ã‚‰è³ªå•ã‚„ä¼šè©±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚(Could not extract questions or conversations.)")
                return None
                
        # Sort questions by timestamp
        actual_questions.sort(key=lambda q: q["segment_start"])
        
        # Return the list of questions
        return {
            "type": "questions",
            "questions": actual_questions
        }
        
    except Exception as e:
        st.error(f"Error extracting questions: {str(e)}")
        st.exception(e)  # Show the full exception for debugging
        return None

# Extract conversations from transcript
def extract_conversations(transcript_segments, min_length=50, max_segments=5):
    """
    Extract complete conversation segments from the transcript
    
    Parameters:
        transcript_segments (list): List of transcript segments
        min_length (int): Minimum length of a conversation in characters
        max_segments (int): Maximum number of conversation segments to extract
    
    Returns:
        list: List of conversation segments
    """
    conversations = []
    
    if not transcript_segments or len(transcript_segments) < 3:
        return conversations
    
    # Group transcript segments into meaningful conversations
    current_conversation = {
        "start_time": transcript_segments[0]["start"],
        "text": transcript_segments[0]["text"],
        "segments": [transcript_segments[0]],
        "end_time": transcript_segments[0]["start"] + transcript_segments[0]["duration"]
    }
    
    # Look for natural breaks in conversation (pauses, speaker changes, etc.)
    for i in range(1, len(transcript_segments)):
        current_segment = transcript_segments[i]
        previous_segment = transcript_segments[i-1]
        
        # Check if there's a significant pause between segments (more than 3 seconds)
        time_gap = current_segment["start"] - (previous_segment["start"] + previous_segment["duration"])
        
        # Check for segment breaks: significant time gap or very different content
        if time_gap > 3.0 or len(current_conversation["text"]) > 500:  # New conversation if gap > 3s or current is already long
            # Save previous conversation if it's long enough
            if len(current_conversation["text"]) >= min_length:
                # Calculate midpoint of conversation for timestamp
                mid_time = (current_conversation["start_time"] + current_conversation["end_time"]) / 2
                current_conversation["timestamp"] = mid_time
                conversations.append(current_conversation)
            
            # Start a new conversation
            current_conversation = {
                "start_time": current_segment["start"],
                "text": current_segment["text"],
                "segments": [current_segment],
                "end_time": current_segment["start"] + current_segment["duration"]
            }
        else:
            # Continue current conversation
            current_conversation["text"] += " " + current_segment["text"]
            current_conversation["segments"].append(current_segment)
            current_conversation["end_time"] = current_segment["start"] + current_segment["duration"]
    
    # Add the last conversation if it's long enough
    if len(current_conversation["text"]) >= min_length:
        # Calculate midpoint of conversation for timestamp
        mid_time = (current_conversation["start_time"] + current_conversation["end_time"]) / 2
        current_conversation["timestamp"] = mid_time
        conversations.append(current_conversation)
    
    # Clean up conversation text and split into meaningful segments if too long
    refined_conversations = []
    for conv in conversations:
        # Clean up text
        text = conv["text"]
        text = re.sub(r'\s+', ' ', text).strip()
        
        # If conversation is very long, try to split it into meaningful segments
        if len(text) > 1000:
            # Look for natural break points like sentence endings
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]\s*', text)
            current_part = ""
            parts = []
            
            for sentence in sentences:
                if not sentence.strip():
                    continue
                
                if len(current_part) < 300:
                    current_part += sentence + "ã€‚ "
                else:
                    parts.append(current_part)
                    current_part = sentence + "ã€‚ "
            
            if current_part:
                parts.append(current_part)
            
            # Create a separate conversation for each meaningful part
            start_offset = 0
            for idx, part in enumerate(parts):
                segment_length = len(part) / len(text)  # Proportion of this segment
                duration = (conv["end_time"] - conv["start_time"]) * segment_length
                
                refined_conversations.append({
                    "start_time": conv["start_time"] + start_offset,
                    "text": part,
                    "timestamp": conv["start_time"] + start_offset + (duration / 2)
                })
                start_offset += duration
        else:
            # For shorter conversations, keep them as is but with cleaned text
            conv["text"] = text
            refined_conversations.append(conv)
    
    # Sort conversations by quality (based on length and completeness)
    # Higher quality conversations tend to be longer and contain complete sentences
    def conversation_quality(conv):
        text = conv["text"]
        # Higher scores for:
        # 1. Length (longer is better, up to a point)
        length_score = min(len(text) / 200, 5)  # Max score of 5 for length
        
        # 2. Completeness (ends with sentence ending)
        completeness_score = 2 if re.search(r'[ã€‚ï¼ï¼Ÿ.!?]\s*$', text) else 0
        
        # 3. Question content (conversations with questions are interesting)
        question_score = text.count('ï¼Ÿ') + text.count('?') + text.count('ã‹ã€‚') + text.count('ã§ã™ã‹') + text.count('ã¾ã™ã‹')
        
        return length_score + completeness_score + question_score
    
    refined_conversations.sort(key=conversation_quality, reverse=True)
    
    # Limit to max_segments
    return refined_conversations[:max_segments]

# Set up page configuration
def setup_page():
    """Set up page configuration and styling"""
    st.set_page_config(
        page_title="Japanese Listening Practice",
        page_icon="ğŸ‡¯ğŸ‡µ",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Add custom styles
    st.markdown("""
    <style>
    .stApp {
        max-width: 1200px;
        margin: 0 auto;
    }
    .main-header {
        color: #1E88E5;
    }
    </style>
    """, unsafe_allow_html=True)

# Render the home page
def render_home():
    """Render the home page"""
    st.title("Japanese Listening Practice")
    
    st.markdown("""
    Welcome to the Japanese Listening Practice app! This application will help you improve your Japanese 
    listening comprehension skills using authentic content from YouTube or AI-generated JLPT-style exercises.
    
    ## Features
    
    - Practice with real Japanese content from YouTube
    - Generate JLPT-style listening exercises with audio
    - Extract questions directly from Japanese YouTube videos
    - Get automatic transcriptions and translations
    - Answer comprehension questions
    - Track your progress over time
    
    ## How to Use
    
    1. Choose the type of practice you want
    2. Listen to the audio and try to understand
    3. Answer the comprehension questions
    4. Review your results and learn from mistakes
    """)
    
    st.write("### Choose Practice Type")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.button("YouTube Videos", on_click=go_to_video_selection, use_container_width=True)
    
    with col2:
        st.button("JLPT-Style Audio Exercises", use_container_width=True)
        
    with col3:
        # Add a button to directly go to the question extraction page
        st.button("Extract Questions from YouTube", on_click=go_to_extract_questions, use_container_width=True)

# Render the video selection page
def render_video_selection():
    """Render the video selection page with YouTube URL input and saved videos"""
    st.title("Select a Video")
    
    # Check if backend is running and show status
    backend_running = st.session_state.get("backend_available", False)
    if not backend_running:
        st.warning("âš ï¸ Backend server is not running - Some features will be limited")
        
    # Add new option for direct question extraction
    with st.expander("Extract Questions from Japanese YouTube Videos", expanded=True):
        st.write("Enter a Japanese YouTube URL to extract natural questions and their context:")
        
        # Add examples of valid YouTube URLs
        st.caption("""
        **Examples of valid YouTube URLs:**
        - https://www.youtube.com/watch?v=XXXXXXXXXXX
        - https://youtu.be/XXXXXXXXXXX
        """)
        
        st.info("This feature works best with Japanese language videos. The system will extract questions in Japanese and provide English translations when possible.")
        
        youtube_url = st.text_input(
            "Japanese YouTube URL", 
            placeholder="https://www.youtube.com/watch?v=...",
            key="direct_extract_url"
        )
        
        col1, col2 = st.columns([3, 1])
        with col2:
            extract_button = st.button("Extract Questions", use_container_width=True)
            
        # Process URL when button is clicked
        if extract_button:
            if not youtube_url:
                st.error("Please enter a YouTube URL")
            else:
                st.session_state["youtube_url"] = youtube_url
                go_to_extract_questions()
    
    # Original custom Video URL Input
    with st.expander("Process with backend (full features)", expanded=False):
        st.write("Enter a YouTube URL to practice with a specific video:")
        
        # Add examples of valid YouTube URLs
        st.caption("""
        **Examples of valid YouTube URLs:**
        - https://www.youtube.com/watch?v=XXXXXXXXXXX
        - https://youtu.be/XXXXXXXXXXX
        - https://m.youtube.com/watch?v=XXXXXXXXXXX
        """)
        
        youtube_url = st.text_input(
            "YouTube URL", 
            placeholder="https://www.youtube.com/watch?v=...",
            help="Paste a YouTube URL with Japanese content for practice"
        )
        
        col1, col2 = st.columns([3, 1])
        with col2:
            submit_button = st.button("Add Video", use_container_width=True)
            
        # Process URL when button is clicked
        if submit_button:
            if not youtube_url:
                st.error("Please enter a YouTube URL")
            else:
                # Show a spinner while processing
                with st.spinner("Processing YouTube URL..."):
                    # Validate URL format
                    youtube_id = extract_youtube_id(youtube_url)
                    if not youtube_id:
                        st.error("Invalid YouTube URL format. Please check the URL and try again.")
                    else:
                        st.info(f"Processing video with ID: {youtube_id}...")
                        # Store URL and redirect to processing page
                        st.session_state.custom_video_url = youtube_url
    
    # Search functionality
    with st.expander("Search for videos", expanded=False):
        search_col1, search_col2 = st.columns([3, 1])
        with search_col1:
            search_query = st.text_input("Search for Japanese videos", placeholder="Enter keywords...")
        with search_col2:
            search_button = st.button("Search", use_container_width=True)
        
        if search_button and search_query:
            st.info(f"Searching for: {search_query}")
            st.warning("Search functionality is under development. Please use the URL input for now.")
    
    # Navigation
    st.button("Back to Home", on_click=go_to_home)

# Render the question extraction page
def render_extract_questions():
    """Render the question extraction page"""
    st.title("YouTubeå‹•ç”»ã‹ã‚‰è³ªå•ãƒ»ä¼šè©±ã‚’æŠ½å‡º (Extract Questions/Conversations from YouTube)")
    
    if "youtube_url" not in st.session_state:
        st.session_state["youtube_url"] = ""
        
    if "extracted_content" not in st.session_state:
        st.session_state["extracted_content"] = None
    
    # Form for URL input
    st.write("### æ—¥æœ¬èªYouTubeã®URLã‚’å…¥åŠ› (Enter a Japanese YouTube URL)")
    youtube_url = st.text_input(
        "æ—¥æœ¬èª YouTube URL", 
        value=st.session_state.get("youtube_url", ""), 
        key="youtube_url_input",
        help="ä¾‹: https://www.youtube.com/watch?v=2aqVJS6QOoY"
    )
    
    # Method selection
    st.write("### æŠ½å‡ºæ–¹æ³•ã‚’é¸æŠ (Select Extraction Method)")
    
    extraction_methods = [
        "å­—å¹•ã‹ã‚‰æŠ½å‡º (Extract from Captions)",
        "AIæŠ½å‡º (Whisper + Ollama)"
    ]
    
    # Default to AI method if Ollama is available, otherwise use captions method
    default_method_idx = 1 if st.session_state.get("ollama_available", False) else 0
    
    # Check if we already have a selected method in the session state
    if "extraction_method" not in st.session_state:
        st.session_state["extraction_method"] = extraction_methods[default_method_idx]
    
    # Display method selection
    col1, col2 = st.columns([3, 1])
    with col1:
        method = st.radio(
            "æŠ½å‡ºæ–¹æ³• (Extraction Method)",
            extraction_methods,
            index=extraction_methods.index(st.session_state["extraction_method"]),
            horizontal=True
        )
        st.session_state["extraction_method"] = method
    
    # Extract button
    with col2:
        extract_btn = st.button("è³ªå•ãƒ»ä¼šè©±ã‚’æŠ½å‡ºã™ã‚‹ (Extract Questions/Conversations)", key="extract_btn", use_container_width=True)
    
    # Display help for methods - removed as per user request
    
    # Check if Ollama is available for AI method
    if method == "AIæŠ½å‡º (Whisper + Ollama)" and not st.session_state.get("ollama_available", False):
        st.warning("""
        âš ï¸ AIæŠ½å‡ºã«ã¯OllamaãŒå¿…è¦ã§ã™ã€‚OllamaãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚
        
        Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã‹ã‚‰ã€`ollama serve`ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
        
        (AI extraction requires Ollama. Ollama is not running. Please install Ollama and run the `ollama serve` command.)
        """)
    
    if extract_btn:
        if not youtube_url:
            st.error("YouTubeã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚(Please enter a YouTube URL.)")
        else:
            st.session_state["youtube_url"] = youtube_url
            with st.spinner("å‹•ç”»ã‹ã‚‰è³ªå•ãƒ»ä¼šè©±ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™... (Extracting content from video...)"):
                try:
                    # Use the selected method
                    if method == "AIæŠ½å‡º (Whisper + Ollama)":
                        if st.session_state.get("ollama_available", False):
                            result = extract_questions_with_ai(youtube_url)
                        else:
                            st.error("Ollama ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€AIæŠ½å‡ºã‚’ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚å­—å¹•ã‹ã‚‰ã®æŠ½å‡ºã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                            result = extract_questions_from_youtube(youtube_url)
                    else:
                        result = extract_questions_from_youtube(youtube_url)
                        
                    if result:
                        st.session_state["extracted_content"] = result
                        if result["type"] == "questions":
                            st.success(f"âœ… æˆåŠŸ! {len(result['questions'])}å€‹ã®è³ªå•ã‚’æŠ½å‡ºã—ã¾ã—ãŸ (Successfully extracted {len(result['questions'])} questions)")
                        else:
                            st.success(f"âœ… æˆåŠŸ! {len(result['conversations'])}å€‹ã®ä¼šè©±ã‚’æŠ½å‡ºã—ã¾ã—ãŸ (Successfully extracted {len(result['conversations'])} conversations)")
                        st.rerun()  # Refresh to display the content
                    else:
                        st.error("ã“ã®å‹•ç”»ã‹ã‚‰è³ªå•ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ä»–ã®å‹•ç”»ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚(Could not extract any questions from this video.)")
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼: {str(e)} (Error processing video)")
                    st.exception(e)
                    st.info("åˆ¥ã®å‹•ç”»ã‚’è©¦ã™ã‹ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚(Try a different video URL or check your internet connection.)")
    
    # Display the video if we have content
    if st.session_state.get("extracted_content") and st.session_state.get("youtube_url"):
        video_id = extract_youtube_id(st.session_state["youtube_url"])
        if video_id:
            st.video(f"https://www.youtube.com/watch?v={video_id}")
    
    # Display content if we have it
    if st.session_state.get("extracted_content"):
        content = st.session_state["extracted_content"]
        
        if content["type"] == "questions":
            # Display questions
            questions = content["questions"]
            
            st.markdown("## å‹•ç”»ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸè³ªå• (Questions Extracted from Video)")
            
            for i, question in enumerate(questions):
                with st.expander(f"è³ªå• {i+1}: {question['question_text'][:30]}...", expanded=True):
                    # Display content type badge
                    st.markdown(f"**{question.get('content_type', 'è³ªå• (Question)')}**")
                    
                    # Display the full question
                    st.subheader(f"{question['question_text']}")
                    
                    # Display context
                    st.markdown("### ä¼šè©±ã®æ–‡è„ˆ (Conversation Context)")
                    
                    if question.get("context_before"):
                        st.markdown("**å‰ (Before):**")
                        st.markdown(f"*{question['context_before']}*")
                    
                    if question.get("context_after"):
                        st.markdown("**å¾Œ (After):**")
                        st.markdown(f"*{question['context_after']}*")
                    
                    # Add timestamp link
                    video_id = extract_youtube_id(st.session_state["youtube_url"])
                    if video_id:
                        # Use timestamp if available, otherwise use segment_start
                        if "timestamp" in question:
                            start_time = int(question["timestamp"])
                        else:
                            start_time = int(question["segment_start"])
                        st.markdown(f"[ã“ã®è³ªå•ã‚’å‹•ç”»ã§è¦‹ã‚‹ (Watch this question in the video)](https://www.youtube.com/watch?v={video_id}&t={start_time}s)")
        else:
            # Display conversations
            conversations = content["conversations"]
            
            st.markdown("## å‹•ç”»ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸä¼šè©± (Conversations Extracted from Video)")
            
            for i, conversation in enumerate(conversations):
                # Prepare a preview of the conversation (first ~30 chars)
                preview = conversation['text'][:30] + "..." if len(conversation['text']) > 30 else conversation['text']
                
                # Use the timestamp for linking to the video if available, otherwise use start_time
                timestamp = conversation.get('timestamp', conversation.get('start_time', 0))
                
                with st.expander(f"ä¼šè©± {i+1}: {preview}", expanded=True):
                    # Display content type badge
                    st.markdown(f"**{conversation.get('content_type', 'ä¼šè©± (Conversation)')}**")
                    
                    # Display the conversation
                    st.markdown("### ä¼šè©±å†…å®¹ (Conversation Content)")
                    st.markdown(f"*{conversation['text']}*")
                    
                    # Add timestamp link
                    video_id = extract_youtube_id(st.session_state["youtube_url"])
                    if video_id:
                        start_time = int(timestamp)
                        st.markdown(f"[ã“ã®ä¼šè©±ã‚’å‹•ç”»ã§è¦‹ã‚‹ (Watch this conversation in the video)](https://www.youtube.com/watch?v={video_id}&t={start_time}s)")
                        
                    # Add additional metadata if available
                    if 'start_time' in conversation and 'end_time' in conversation:
                        duration = conversation.get('end_time', 0) - conversation.get('start_time', 0)
                        if duration > 0:
                            st.caption(f"ä¼šè©±ã®æ™‚é–“: {int(conversation['start_time'])}ç§’ - {int(conversation['end_time'])}ç§’ (ç´„{int(duration)}ç§’é–“)")
    
    # Navigation buttons
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ãƒ›ãƒ¼ãƒ ã«æˆ»ã‚‹ (Return to Home)", use_container_width=True):
            # Clear extraction results
            if "extracted_content" in st.session_state:
                del st.session_state["extracted_content"]
            go_to_home()
    with col2:
        if st.button("åˆ¥ã®å‹•ç”»ã‚’é¸ã¶ (Select Another Video)", use_container_width=True):
            # Clear extraction results
            if "extracted_content" in st.session_state:
                del st.session_state["extracted_content"]
            go_to_video_selection()

# Extract questions using Whisper + Llama
def extract_questions_with_ai(video_url):
    """
    Extract questions from a YouTube video using Whisper for transcription
    and a local LLM for question extraction and analysis
    
    Parameters:
        video_url (str): YouTube URL
    
    Returns:
        dict: Dictionary with extracted content (questions with context)
    """
    try:
        # Validate the YouTube URL
        video_id = extract_youtube_id(video_url)
        if not video_id:
            st.error("ç„¡åŠ¹ãªYouTube URLã§ã™ã€‚æœ‰åŠ¹ãªURLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚(Invalid YouTube URL. Please provide a valid URL.)")
            return None
        
        st.info(f"å‡¦ç†ä¸­ã®ãƒ“ãƒ‡ã‚ªID: {video_id} (Processing video ID: {video_id})")
        
        # Check if Ollama is available
        if not st.session_state.get("ollama_available", False):
            st.warning("Ollama ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã“ã®æ©Ÿèƒ½ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚(This feature requires Ollama to be available.)")
            return None
        
        # Step 1: Download audio from YouTube
        with st.spinner("YouTubeã‹ã‚‰éŸ³å£°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­... (Downloading audio from YouTube...)"):
            try:
                # Check if yt-dlp is installed
                try:
                    import subprocess
                    subprocess.run(["yt-dlp", "--version"], capture_output=True, check=True)
                except (FileNotFoundError, subprocess.CalledProcessError):
                    st.error("yt-dlp ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install yt-dlp` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚(yt-dlp is not installed. Please run `pip install yt-dlp`.)")
                    return None
                
                # Create a temporary directory for downloaded files
                import tempfile
                import os
                
                temp_dir = tempfile.mkdtemp()
                audio_file = os.path.join(temp_dir, f"{video_id}.mp3")
                
                # Download audio only
                cmd = [
                    "yt-dlp", 
                    "-x", "--audio-format", "mp3", 
                    "--audio-quality", "0",
                    "-o", audio_file,
                    f"https://www.youtube.com/watch?v={video_id}"
                ]
                
                process = subprocess.run(cmd, capture_output=True, text=True)
                
                if process.returncode != 0:
                    st.error(f"éŸ³å£°ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {process.stderr} (Failed to download audio)")
                    return None
                
                if not os.path.exists(audio_file):
                    # Check if .webm file was created instead
                    webm_file = os.path.join(temp_dir, f"{video_id}.webm")
                    if os.path.exists(webm_file):
                        audio_file = webm_file
                    else:
                        # Look for any audio file
                        files = os.listdir(temp_dir)
                        for file in files:
                            if file.startswith(video_id) and os.path.isfile(os.path.join(temp_dir, file)):
                                audio_file = os.path.join(temp_dir, file)
                                break
                        
                        if not os.path.exists(audio_file):
                            st.error("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚(Audio file not found.)")
                            return None
                
                st.success(f"éŸ³å£°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {audio_file} (Audio download complete)")
            
            except Exception as e:
                st.error(f"éŸ³å£°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)} (Error during audio download)")
                return None
        
        # Step 2: Transcribe audio using Whisper
        with st.spinner("Whisperã§éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—ä¸­... (Transcribing audio with Whisper...)"):
            try:
                # Check if whisper is installed
                try:
                    import whisper
                except ImportError:
                    st.error("Whisper ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install openai-whisper` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚(Whisper is not installed. Please run `pip install openai-whisper`.)")
                    return None
                
                # Load the Whisper model
                model_size = "base"  # Options: tiny, base, small, medium, large
                model = whisper.load_model(model_size)
                
                # Transcribe the audio
                result = model.transcribe(audio_file, language="ja")
                transcript = result["text"]
                
                if not transcript:
                    st.error("æ–‡å­—èµ·ã“ã—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚(Transcription failed.)")
                    return None
                
                # Get segments with timestamps
                segments = result["segments"]
                
                # Display the transcript
                with st.expander("æ–‡å­—èµ·ã“ã—çµæœ (Transcript)", expanded=False):
                    st.write(transcript)
                    st.write("---")
                    st.write("### ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ (Segments)")
                    for segment in segments:
                        st.write(f"{segment['start']:.1f}s - {segment['end']:.1f}s: {segment['text']}")
                
                st.success(f"æ–‡å­—èµ·ã“ã—å®Œäº†: {len(transcript)} æ–‡å­— (Transcription complete: {len(transcript)} characters)")
                
            except Exception as e:
                st.error(f"æ–‡å­—èµ·ã“ã—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)} (Error during transcription)")
                st.exception(e)
                return None
        
        # Step 3: Use Llama model to extract questions and context
        with st.spinner("LLMã§è³ªå•ã¨æ–‡è„ˆã‚’æŠ½å‡ºä¸­... (Extracting questions and context with LLM...)"):
            try:
                # Prepare the system prompt
                system_prompt = """
                ã‚ãªãŸã¯æ—¥æœ¬èªã®ä¼šè©±ã‹ã‚‰è³ªå•ã¨ä¼šè©±ã®æ–‡è„ˆã‚’æŠ½å‡ºã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
                ä»¥ä¸‹ã®æ–‡å­—èµ·ã“ã—ã‹ã‚‰ã€å®Ÿéš›ã«è©±ã—æ‰‹ãŒç™ºã—ãŸè³ªå•ã‚’æŠ½å‡ºã—ã€å„è³ªå•ã®å‰å¾Œã®æ–‡è„ˆã‚‚å«ã‚ã¦ãƒªã‚¹ãƒˆã«ã—ã¦ãã ã•ã„ã€‚

                è³ªå•ã®åŸºæº–:
                - ã€Œã‹ï¼Ÿã€ã€Œã§ã™ã‹ï¼Ÿã€ã€Œã¾ã™ã‹ï¼Ÿã€ãªã©ã§çµ‚ã‚ã‚‹æ–‡
                - ç–‘å•è©ï¼ˆä½•ã€ã©ã†ã€ãªãœã€ã©ã“ã€èª°ã€ã„ã¤ã€ã©ã‚“ãªï¼‰ã‚’å«ã‚€æ–‡ã§ã€è³ªå•ã®æ„å›³ãŒã‚ã‚‹ã‚‚ã®
                - æ˜ã‚‰ã‹ã«è³ªå•ã¨ã—ã¦ç™ºã›ã‚‰ã‚ŒãŸæ–‡

                å‡ºåŠ›å½¢å¼:
                {
                    "questions": [
                        {
                            "question_text": "è³ªå•ã®æ–‡å­—åˆ—",
                            "context_before": "è³ªå•ã®å‰ã®æ–‡è„ˆ",
                            "context_after": "è³ªå•ã®å¾Œã®æ–‡è„ˆ",
                            "timestamp": "è³ªå•ãŒç™ºã›ã‚‰ã‚ŒãŸãŠãŠã‚ˆãã®æ™‚é–“ï¼ˆç§’ï¼‰"
                        },
                        ...
                    ]
                }

                è³ªå•ãŒãªã„å ´åˆã¯ã€ä»£ã‚ã‚Šã«ä¼šè©±ã®é‡è¦ãªéƒ¨åˆ†ã‚’ä»¥ä¸‹ã®å½¢å¼ã§æŠ½å‡ºã—ã¦ãã ã•ã„:
                {
                    "conversations": [
                        {
                            "text": "ä¼šè©±ã®é‡è¦ãªéƒ¨åˆ†",
                            "timestamp": "ä¼šè©±ãŒç™ºã›ã‚‰ã‚ŒãŸãŠãŠã‚ˆãã®æ™‚é–“ï¼ˆç§’ï¼‰"
                        },
                        ...
                    ]
                }

                æ³¨æ„:
                - è³ªå•ãŒå°‘ãªãã¨ã‚‚1ã¤ã‚ã‚‹å ´åˆã¯ questions å½¢å¼ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
                - è³ªå•ãŒãªã„å ´åˆã®ã¿ conversations å½¢å¼ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
                - JSONã¨ã—ã¦æœ‰åŠ¹ãªå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„
                - å®Ÿéš›ã®è³ªå•ã®ã¿ã‚’æŠ½å‡ºã—ã€æ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼ã‚„ä¸æ˜ç­ãªéƒ¨åˆ†ã¯ç„¡è¦–ã—ã¦ãã ã•ã„
                - å„è³ªå•ã«ã¯ååˆ†ãªæ–‡è„ˆï¼ˆå‰å¾Œå°‘ãªãã¨ã‚‚1-2æ–‡ï¼‰ã‚’å«ã‚ã¦ãã ã•ã„
                """
                
                # Get the selected model
                model = st.session_state.get("ollama_model", "llama3.2:1b")
                
                # Prepare the transcript (truncate if too long)
                max_length = 6000  # Avoid context length issues
                if len(transcript) > max_length:
                    # Try to find a good break point
                    truncated_transcript = transcript[:max_length]
                    last_period = truncated_transcript.rfind('ã€‚')
                    if last_period > max_length * 0.8:  # At least 80% of the max length
                        truncated_transcript = truncated_transcript[:last_period+1]
                    transcript = truncated_transcript
                    st.warning(f"æ–‡å­—èµ·ã“ã—ãŒé•·ã™ãã‚‹ãŸã‚ã€æœ€åˆã®{len(truncated_transcript)}æ–‡å­—ã®ã¿ã‚’åˆ†æã—ã¾ã™ã€‚(Transcript is too long, analyzing only the first {len(truncated_transcript)} characters.)")
                
                # Call Ollama with the transcript
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json={
                        "model": model,
                        "prompt": transcript,
                        "system": system_prompt,
                        "stream": False,
                        "format": "json"
                    },
                    timeout=60  # Increased timeout for longer processing
                )
                
                if response.status_code != 200:
                    st.error(f"LLMã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¨ãƒ©ãƒ¼: {response.status_code} (LLM response error)")
                    return None
                
                result = response.json()
                ai_response = result.get('response', '')
                
                # Try to parse the JSON from the response
                import json
                try:
                    # Find JSON in the response
                    json_start = ai_response.find('{')
                    json_end = ai_response.rfind('}') + 1
                    
                    if json_start >= 0 and json_end > json_start:
                        json_str = ai_response[json_start:json_end]
                        content = json.loads(json_str)
                    else:
                        # Try to parse the entire response
                        content = json.loads(ai_response)
                        
                    # Check if we have questions or conversations
                    if "questions" in content and content["questions"]:
                        # Add content type to each question
                        for question in content["questions"]:
                            question["content_type"] = "è³ªå• (Question)"
                            # Ensure timestamp is an int or float
                            if "timestamp" in question and question["timestamp"]:
                                try:
                                    question["timestamp"] = float(question["timestamp"])
                                    question["segment_start"] = question["timestamp"]
                                    question["segment_end"] = question["timestamp"] + 5  # Approximate 5 seconds for the question
                                except:
                                    # If timestamp can't be converted, use 0
                                    question["timestamp"] = 0
                                    question["segment_start"] = 0
                                    question["segment_end"] = 5
                                    
                        st.success(f"âœ… æˆåŠŸ! {len(content['questions'])}å€‹ã®è³ªå•ã‚’æŠ½å‡ºã—ã¾ã—ãŸ (Successfully extracted {len(content['questions'])} questions)")
                        return {
                            "type": "questions",
                            "questions": content["questions"]
                        }
                    elif "conversations" in content and content["conversations"]:
                        # Add content type to each conversation
                        for conversation in content["conversations"]:
                            conversation["content_type"] = "ä¼šè©± (Conversation)"
                            # Ensure timestamp is an int or float
                            if "timestamp" in conversation and conversation["timestamp"]:
                                try:
                                    conversation["timestamp"] = float(conversation["timestamp"])
                                    conversation["start_time"] = conversation["timestamp"]
                                    conversation["end_time"] = conversation["timestamp"] + 10  # Approximate 10 seconds for the conversation
                                except:
                                    # If timestamp can't be converted, use 0
                                    conversation["timestamp"] = 0
                                    conversation["start_time"] = 0
                                    conversation["end_time"] = 10
                        
                        st.success(f"âœ… æˆåŠŸ! {len(content['conversations'])}å€‹ã®ä¼šè©±ã‚’æŠ½å‡ºã—ã¾ã—ãŸ (Successfully extracted {len(content['conversations'])} conversations)")
                        return {
                            "type": "conversations",
                            "conversations": content["conversations"]
                        }
                    else:
                        st.error("LLMã‹ã‚‰ã®å¿œç­”ã«è³ªå•ã¾ãŸã¯ä¼šè©±ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚(No questions or conversations in LLM response.)")
                        with st.expander("LLMã‹ã‚‰ã®å¿œç­” (LLM Response)", expanded=False):
                            st.code(ai_response)
                        return None
                
                except json.JSONDecodeError as e:
                    st.error(f"JSONã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)} (Failed to parse JSON)")
                    with st.expander("LLMã‹ã‚‰ã®å¿œç­” (LLM Response)", expanded=False):
                        st.code(ai_response)
                    return None
                
            except Exception as e:
                st.error(f"è³ªå•æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)} (Error during question extraction)")
                st.exception(e)
                return None
            
        # Clean up
        try:
            import shutil
            shutil.rmtree(temp_dir)
        except:
            pass
        
    except Exception as e:
        st.error(f"Error in AI extraction process: {str(e)}")
        st.exception(e)
        return None

# Main app function
def main():
    """Main application function"""
    # Set up page
    setup_page()
    
    # Add a startup placeholder for initial loading
    startup_placeholder = st.empty()
    with startup_placeholder:
        st.info("Starting application...")
    
    try:
        # Initialize session state
        initialize_session_state()
            
        # Check if backend is running
        backend_url = find_backend_server()
        st.session_state["backend_available"] = backend_url is not None
        st.session_state["backend_url"] = backend_url
        
        # Check if Ollama is available
        if "ollama_available" not in st.session_state:
            check_ollama_availability()
        
        # Clear the startup placeholder once initialization is complete
        startup_placeholder.empty()
        
        # Render the appropriate page based on app_state
        if st.session_state["app_state"] == APP_STATES["HOME"]:
            render_home()
        elif st.session_state["app_state"] == APP_STATES["VIDEO_SELECTION"]:
            render_video_selection()
        elif st.session_state["app_state"] == APP_STATE_EXTRACT_QUESTIONS:
            render_extract_questions()
        # Additional pages can be added here as they are implemented
        else:
            # For now, any other state will default to home
            st.session_state["app_state"] = APP_STATES["HOME"]
            render_home()
            
    except Exception as e:
        # Provide a friendly error message and recovery options
        st.error(f"An error occurred in the application: {str(e)}")
        
        # Show detailed error in an expander for debugging
        with st.expander("Error Details", expanded=False):
            st.code(traceback.format_exc())
            
        # Offer recovery options
        st.warning("You can try to restart the application or reload the page.")
    
    # Sidebar with app information
    with st.sidebar:
        st.title("Japanese Listening Practice")
        st.markdown("---")
        
        # Display backend status
        if st.session_state.get("backend_available", False):
            st.success("âœ… Backend server is running")
        else:
            st.warning("âš ï¸ Backend server is not running")
            st.info("Some features may be limited without a backend server.")
        
        # Display Ollama status with retry button
        if st.session_state.get("ollama_available", False):
            st.success("âœ… Ollama is available")
            # Display model selection
            current_model = st.session_state.get("ollama_model", "llama3.2:1b")
            
            # Standard models that might be available
            standard_models = ["llama3.2:1b", "llama3", "mistral", "gemma", "codellama", "phi3", "wizardcoder", "solar", "qwen"]
            
            # Try to get available models from Ollama
            available_models = []
            try:
                response = requests.get("http://localhost:11434/api/tags", timeout=1)
                if response.status_code == 200:
                    models_data = response.json()
                    available_models = [model["name"] for model in models_data.get("models", [])]
            except:
                pass
            
            # Combine standard models with any discovered models
            model_options = list(set(standard_models + available_models))
            
            # If current model isn't in the list, add it
            if current_model not in model_options:
                model_options.append(current_model)
                
            # Sort the model options
            model = st.selectbox(
                "Selected model:",
                model_options,
                index=model_options.index(current_model) if current_model in model_options else 0
            )
            
            # Update model in session state
            if model != st.session_state.get("ollama_model"):
                st.session_state["ollama_model"] = model
                st.info(f"Model changed to {model}. This will be used for translations.")
                
            # Show a button to test the model
            if st.button("Test Translation"):
                with st.spinner("Testing model..."):
                    try:
                        test_text = "ã“ã‚“ã«ã¡ã¯ã€å…ƒæ°—ã§ã™ã‹ï¼Ÿ"
                        response = requests.post(
                            "http://localhost:11434/api/generate",
                            json={
                                "model": model,
                                "prompt": f"Translate this Japanese text to English: {test_text}",
                                "system": "You are a translator that translates Japanese to English.",
                                "stream": False
                            },
                            timeout=5
                        )
                        if response.status_code == 200:
                            translation = response.json().get('response', '').strip()
                            st.success(f"Translation successful! ({test_text} â†’ {translation})")
                        else:
                            st.error(f"Error testing model: HTTP {response.status_code}")
                    except Exception as e:
                        st.error(f"Error testing model: {str(e)}")
        else:
            st.warning("âš ï¸ Ollama is not available")
            st.info("Ollama provides AI models for translation and analysis. Translation features will be limited without it.")
            
            # Add instructions to start Ollama
            with st.expander("How to start Ollama", expanded=True):
                st.markdown("""
                1. Open a terminal/command prompt
                2. Run the command: `ollama serve`
                3. Wait for Ollama to start
                4. Click the 'Retry Ollama Connection' button below
                """)
            
            # Add retry button
            if st.button("Retry Ollama Connection"):
                with st.spinner("Checking Ollama connection..."):
                    check_ollama_availability()
                if st.session_state.get("ollama_available", False):
                    st.success("âœ… Successfully connected to Ollama!")
                    st.info("Please refresh the page to apply changes.")
                else:
                    st.error("âŒ Still unable to connect to Ollama.")
                    st.info("Ensure Ollama is running with the command 'ollama serve' in a terminal window.")
        
        st.markdown("---")
        st.markdown("### About")
        st.markdown("""
        This app helps Japanese learners practice 
        listening comprehension through YouTube videos
        and AI-generated exercises.
        """)
        
        st.markdown("### Version")
        st.markdown("v1.0.0")

# Run the app
if __name__ == "__main__":
    main() 