# Japanese Listening Learning App

A fully local and open-source application for Japanese language learners to practice listening comprehension through interactive exercises.

## Overview

The Japanese Listening Learning App helps Japanese language learners improve their listening comprehension skills through:
- Interactive practice with authentic Japanese content from YouTube videos
- AI-generated JLPT-style listening exercises with realistic audio
- Question extraction and generation using local AI models
- Local storage of practice data and learning progress
- User-friendly interface for consistent and effective practice

## Key Features

- **100% Local Processing**: All processing happens on your local machine for privacy and control
- **YouTube Integration**: Extract content from YouTube for authentic Japanese listening materials
- **JLPT-Style Listening Practice**: Generate and practice with realistic JLPT-format audio exercises
- **Question Extraction Methods**:
  - Caption-based extraction: Fast extraction using YouTube captions
  - AI-powered extraction: High-quality extraction using Whisper and Ollama
- **Automatic Speech Recognition**: High-quality transcription using local Whisper ASR
- **AI-Generated Practice Content**: Questions and exercises generated by locally-hosted Ollama models
- **Multi-Speaker Audio Synthesis**: Dialogues with distinct voices for authentic listening experience
- **Text-to-Speech Options**: Both offline (pyttsx3) and online (gTTS) TTS capabilities
- **Progress Tracking**: Local database to track your learning history and progress
- **Responsive UI**: User-friendly Streamlit interface with intuitive navigation

## Technical Architecture

- **Frontend**: Streamlit-based user interface (Port 8506 by default)
- **Backend**: FastAPI server for business logic and database operations (Port 8040 by default)
- **ASR Engine**: OpenAI Whisper (local implementation, no API keys needed)
- **LLM Processing**: Ollama for local large language model inference
- **TTS Systems**: 
  - Primary: pyttsx3 for offline text-to-speech
  - Fallback: gTTS for online text-to-speech when needed
- **Audio Processing**: FFmpeg for combining and manipulating audio segments
- **Data Storage**: SQLite3 database for persistent storage
- **YouTube Integration**: youtube-transcript-api and yt-dlp for video processing

## Directory Structure

```
Listening_Learning_App/
├── frontend/           # Streamlit UI components
│   ├── main.py         # Main frontend application
│   ├── pages/          # Additional UI pages
│   └── utils/          # Frontend utility functions
├── backend/            # FastAPI server and API endpoints
├── utils/              # Shared utilities (TTS, audio processing)
├── data/               # Storage for database and generated content
├── models/             # AI model configurations
├── run.py              # Main application runner
├── run_app.py          # Application launcher with port management
├── config.json         # Application configuration
├── requirements.txt    # Python dependencies
├── start.sh            # Linux/macOS startup script
└── README.md           # This documentation
```

## System Requirements

- **Operating System**: Windows, macOS, or Linux
- **Python**: Python 3.9+ (3.10 recommended)
- **RAM**: 4GB minimum, 8GB+ recommended for smoother AI processing
- **Disk Space**: 
  - ~2GB for application and dependencies
  - ~2GB for AI models (varies based on Whisper model size)
- **External Dependencies**:
  - FFmpeg: Required for audio processing
  - Ollama: Required for local LLM capabilities

## Installation

### Prerequisites

1. **Python 3.9+**: [Download Python](https://www.python.org/downloads/)
2. **FFmpeg**: 
   - Windows: [Download from ffmpeg.org](https://ffmpeg.org/download.html) and add to PATH
   - macOS: `brew install ffmpeg`
   - Linux: `sudo apt install ffmpeg`
3. **Ollama**:
   - [Download from ollama.ai](https://ollama.ai/) and install
   - Run `ollama serve` to start the Ollama server
   - Pull the recommended model: `ollama pull llama3.2:1b` (smaller, fast) or `ollama pull llama3` (larger, better quality)

### Installation Steps

1. **Clone or download the repository**:
   ```bash
   git clone https://github.com/yourusername/free-genai-bootcamp-2025.git
   cd free-genai-bootcamp-2025/Listening_Learning_App
   ```

2. **Set up a virtual environment** (recommended):
   ```bash
   # Create virtual environment
   python -m venv venv
   
   # Activate virtual environment
   # On Windows:
   venv\Scripts\activate
   # On macOS/Linux:
   source venv/bin/activate
   ```

3. **Install required dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Additional step for Linux users** (for better local TTS):
   ```bash
   sudo apt-get install espeak
   ```

## Starting the Application

### Automatic Start (Recommended)

- **On Linux/macOS**:
  ```bash
  chmod +x start.sh  # Make the script executable (first time only)
  ./start.sh
  ```

- **On Windows**:
  ```
  python run.py --auto-install
  ```

### Manual Start with Options

Start the complete application:
```bash
python run.py
```

Additional startup options:
```bash
python run.py --auto-install     # Auto-install missing dependencies
python run.py --skip-checks      # Skip dependency verification
python run.py --frontend-only    # Run only the frontend
python run.py --backend-only     # Run only the backend API server
python run.py --port 8080        # Specify a custom port for the frontend
python run.py --backend-port 8050  # Specify a custom port for the backend
python run.py --ollama-model llama3  # Specify which Ollama model to use
python run.py --whisper-model medium  # Specify which Whisper model to use
```

## Usage Guide

### YouTube Video Practice

1. Start the app and select "YouTube Videos" from the home page
2. Enter a YouTube URL containing Japanese content
3. Choose your extraction method:
   - **字幕から抽出 (Extract from Captions)**: Faster, depends on caption quality
   - **AI抽出 (Whisper + Ollama)**: Higher quality, requires more processing time
4. Click "Extract Questions/Conversations" to process the video
5. After processing, answer the comprehension questions
6. Submit your answers to see your results and feedback

### JLPT-Style Audio Exercises

1. Start the app and select "JLPT-Style Audio Exercises" from the home page
2. Choose one of the options:
   - **Create new exercise**: Generate a custom exercise
   - **Load existing exercise**: Use a previously created exercise
3. For new exercises, configure:
   - JLPT level (N5-N1)
   - Optional topic (e.g., restaurant, travel, school)
   - Number of questions (1-10)
   - Speaker genders for the dialogue
4. The app will generate:
   - A realistic JLPT-style listening script
   - Audio with different voices for each speaker
   - Multiple-choice comprehension questions
5. Listen to the audio and answer the questions
6. Submit to receive your score and review

### Custom Audio Practice (Advanced)

1. Select "Custom Audio Practice" from the home page
2. Upload your own audio file or provide a text script
3. Configure the exercise settings
4. The app will generate questions based on your custom content
5. Practice with the generated materials

## Configuration

The application settings can be customized by editing the `config.json` file:

```json
{
  "frontend_port": 8506,        // Port for the Streamlit UI
  "backend_port": 8047,         // Port for the FastAPI backend
  "ollama_model": "llama3",     // Ollama model to use
  "whisper_model": "base",      // Whisper model size (tiny, base, small, medium, large)
  "log_level": "INFO"           // Logging verbosity
}
```

## Troubleshooting

### Ollama Issues

- **Ollama Not Running**:
  - Start Ollama with `ollama serve` in a separate terminal
  - Keep this terminal open while using the app

- **Model Not Found**:
  - Pull the model manually: `ollama pull llama3.2:1b`
  - Check available models: `ollama list`

- **Memory Issues**:
  - Use smaller models: `llama3.2:1b` (1.3GB) instead of `llama3` (4.7GB)
  - Close other memory-intensive applications
  - Add more RAM or enable swap space

### Audio and TTS Issues

- **FFmpeg Not Found**:
  - Ensure FFmpeg is installed and in your PATH
  - Verify with `ffmpeg -version`

- **TTS Problems on Linux**:
  - Install espeak: `sudo apt-get install espeak`
  - The app will fallback to online gTTS if offline TTS fails

- **Audio Not Playing**:
  - Check browser audio settings
  - Try using headphones or external speakers
  - Ensure audio files are properly generated in the data directory

### Connection Issues

- **Backend Connection Failed**:
  - Check if the backend server is running
  - Verify the ports in config.json aren't being used by other applications
  - Try restarting the application with `python run.py`

- **Port Already in Use**:
  - The app will automatically try to find free ports
  - Manually specify ports with `--port` and `--backend-port` options

## For Developers

### API Endpoints

The backend provides several API endpoints:

- **GET /health**: Check if the backend is running
- **POST /extract-questions**: Extract questions from video
- **POST /generate-audio**: Generate JLPT-style audio exercise
- **GET /exercises**: List available exercises
- **GET /exercises/{exercise_id}**: Get specific exercise details

### Adding New Features

1. Frontend changes should be made in `frontend/main.py` or `frontend/pages/`
2. Backend API endpoints can be added to the FastAPI server in `backend/`
3. Update the README.md and documentation when adding significant features

## Acknowledgements

This application uses several open-source technologies:
- Whisper by OpenAI for speech recognition
- Ollama for local LLM capabilities
- Streamlit and FastAPI for the interface
- Various Python libraries as listed in requirements.txt

## Future Enhancements

- Support for additional languages beyond Japanese
- More sophisticated progress tracking and spaced repetition
- Integration with language learning databases for vocabulary extraction
- Improved UI/UX with more customization options
- Mobile-friendly version

## License

This project is open-source and free to use. Contributions are welcome! 